# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F__l0f13UJyWqVaDxvMbL0RzLt7n9j1a
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

data_original = pd.read_csv('../data/TBI PUD 10-08-2013.csv')

import data_cleaning

data = data_cleaning.exclude_rows(data_original)
data = data_cleaning.drop_unrelated_features(data)
data = data_cleaning.drop_repetitive_features(data)
data = data_cleaning.replace_na(data)

data_younger = data[data['AgeTwoPlus'] == 1]
data_older = data[data['AgeTwoPlus'] == 2]

"""## Younger Patients (<2 Years)"""

np.random.seed(42)
# 2Ô∏è‚É£ Split data into features and target
X = data_younger.drop(columns=["PosIntFinal", "PatNum"])
y = data_younger["PosIntFinal"]

# Split into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 3Ô∏è‚É£ Train an initial Decision Tree Classifier
tree = DecisionTreeClassifier(max_depth=10, random_state=42, class_weight={0:1, 1:80})
tree.fit(X_train, y_train)

# Analyze feature importance
feature_importances = pd.DataFrame({"Feature": X.columns, "Importance": tree.feature_importances_})
feature_importances = feature_importances.sort_values(by="Importance", ascending=False)
print("\nFeature Importance (Initial Decision Tree):\n", feature_importances)

# 4Ô∏è‚É£ Select important features using Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight={0:1, 1:80})
rf.fit(X_train, y_train)

selector = SelectFromModel(rf, threshold="mean", prefit=True)  # Select features above the mean importance
selected_features = X.columns[selector.get_support()]
print("\nSelected Features for Final Model:", list(selected_features))

# 5Ô∏è‚É£ Retrain Decision Tree with only selected features
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

tree_selected = DecisionTreeClassifier(max_depth=5, random_state=42, class_weight={0:1, 1:80},
                                       min_samples_leaf=500, min_samples_split=800)
tree_selected.fit(X_train_selected, y_train)

# 6Ô∏è‚É£ Evaluate the model
y_pred = tree_selected.predict(X_test_selected)

print("\nClassification Report (Final Decision Tree):\n", classification_report(y_test, y_pred))
print("\nAccuracy Score:", accuracy_score(y_test, y_pred))

# üéØ Plot final decision tree
plt.figure(figsize=(12, 7))
plot_tree(tree_selected, feature_names=selected_features, class_names=["No ciTBI", "ciTBI"], filled=True, fontsize=8)
plt.title("Decision Tree After Feature Selection")
plt.savefig("tree_younger.svg")
plt.show()

"""## Older Patients (>=2 Years)"""

np.random.seed(42)
# 2Ô∏è‚É£ Split data into features and target
X = data_older.drop(columns=["PosIntFinal", "PatNum"])
y = data_older["PosIntFinal"]

# Split into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 3Ô∏è‚É£ Train an initial Decision Tree Classifier
tree = DecisionTreeClassifier(max_depth=10, random_state=42, class_weight={0:1, 1:80})
tree.fit(X_train, y_train)

# Analyze feature importance
feature_importances = pd.DataFrame({"Feature": X.columns, "Importance": tree.feature_importances_})
feature_importances = feature_importances.sort_values(by="Importance", ascending=False)
print("\nFeature Importance (Initial Decision Tree):\n", feature_importances)

# 4Ô∏è‚É£ Select important features using Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight={0:1, 1:80})
rf.fit(X_train, y_train)

selector = SelectFromModel(rf, threshold="mean", prefit=True)  # Select features above the mean importance
selected_features = X.columns[selector.get_support()]
print("\nSelected Features for Final Model:", list(selected_features))

# 5Ô∏è‚É£ Retrain Decision Tree with only selected features
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

tree_selected = DecisionTreeClassifier(max_depth=5, random_state=42, class_weight={0:1, 1:80},
                                       min_samples_leaf=800, min_samples_split=3000)
tree_selected.fit(X_train_selected, y_train)

# 6Ô∏è‚É£ Evaluate the model
y_pred = tree_selected.predict(X_test_selected)

print("\nClassification Report (Final Decision Tree):\n", classification_report(y_test, y_pred))
print("\nAccuracy Score:", accuracy_score(y_test, y_pred))

# üéØ Plot final decision tree
plt.figure(figsize=(12, 6))
plot_tree(tree_selected, feature_names=selected_features, class_names=["No ciTBI", "ciTBI"], filled=True)
plt.title("Decision Tree After Feature Selection")
plt.savefig("tree_older.svg")
plt.show()