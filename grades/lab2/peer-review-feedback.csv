group,Skeleton/Structure - Check the box for each file that is included in the folder.,Skeleton/Structure Deductions - Check the box for each issue found.,Code Style - check the applicable boxes,"Reproducibility of report. You don't have to actually run anything, but is it clear to you, based on what is in the ""code"" folder, how all the figures/results in the report were produced? Would it be easy to reproduce them? If not, discuss why.

(Since I'm not asking you to run anything, I haven't provided any additional items students may have put in their ""data"" folders, but if e.g. the code refers to '../data/some-input-file.csv', please assume it exists)",How easy would it be for you to use their code to train models in the same way they did? Are the relevant scripts easy to use? Is the overall code structure made clear?,Readability of report (Was the narrative clear and easy to read? Or did you find it hard to follow? Any grammar mistakes?),Relevance of figures - excluding findings (were the figures relevant and discussed in the report?),"Quality of figures (Were the figures easy to understand? Were there captions? Were the axes labeled? Were they visually appealing? If not, what would you have changed?)",Autoencoder Training Experiments (How much effort did they put into tuning the autoencoder hyperparameters?),Autoencoder Architecture Modification (How meaningful was the change in the architecture?),Description of Autoencoder Implementation (Discuss the autoencoder section of their report. How well did they document and describe their training procedure? Did their decisions seem reasonable? Are you convinced that their final model did a good job of encoding the data?),"Model 1 Implementation
(Discuss the model implementation. How much effort did they put into model tuning? Did they pick a model that was appropriate for the task? Did they do anything clever?)","Model 1 Justification
(Discuss how well they justified the choices made while implementing the model.)","Model 2 Implementation
(Discuss the model implementation. How much effort did they put into model tuning? Did they pick a model that was appropriate for the task? Did they do anything clever?)","Model 2 Justification
(Discuss how well they justified the choices made while implementing the model.)","Model 3 Implementation
(Discuss the model implementation. How much effort did they put into model tuning? Did they pick a model that was appropriate for the task? Did they do anything clever?)","Model 3 Justification
(Discuss how well they justified the choices made while implementing the model.)",Discuss the stability check (Was the check relevant to the final model performance? Did it help to convince you that the model would perform well in new situations or that a different data scientist would have produced similar results?),Discuss the final model evaluation (Were you convinced that their final model was the best of the three and that it properly evaluated? Was their process clearly described?),Additional comments/feedback?
3,"environment.yaml, lab2.tex or lab2.ipynb, lab2.pdf, Autoencoder checkpoint file in a `results` directory.",None of the above,"Sufficient comments are included to understand code., Sufficient docstrings are included to understand APIs and functions., Consistent style is used.",It's easy to reproduce.,"Their approach is easy to follow, they discuss how to run scripts in the report.",Clear and easy to read.,Figures are relevant and discussed,The only issue is that their figures have no captions.,3,3,"They discuss how they implement autoencoder, modify the architecture and test multiple hyperparameters. However, they didn't provide results like validation loss and visualizations to support their choice of best architecture/parameters.",3,"They described their method and how they tune the model, but they didn't provide enough evidence to evaluate the model's performance.",3,Same problem as model1,3,Same problem as model1,They add Gaussian noise to the dataset and apply models on perturbed data as stability check. I think this stability check is relevant and the result is convincing.,They didn't provide enough results to evaluate performances of three classifiers.  ,They claimed they applied the model to unlabeled data but they didn't provide any visualizations to show how it looks like.
3,"environment.yaml, lab2.tex or lab2.ipynb, lab2.pdf, Autoencoder checkpoint file in a `results` directory.",None of the above,"Sufficient comments are included to understand code., Sufficient docstrings are included to understand APIs and functions., Consistent style is used.","i dont see a run.sh, so not quite sure how I would run their code.","there is a job.sh to run the autoencoder, so that seems like it might be easy to use. they don't make a comment saying it only works on the psc cluster though.","Well written, no big gramatical mistakes.",Yes all seemed relevant.,All the figures were easy to understand. If they weren't then the caption or text explained them well.,3,3,"They described it very well i words, but I wish there was a simple diagram so i didn't have to read all of it to get an overview. They showed a picture of the autoencoder embedding to give an idea of how it encoded the data.",4,well justified. also explained what the model does briefly.,4,"similar to model 1, choice seemed justified with a brief explanation of the model and performance.",4,same as above.,"They added gaussian noise and rerran their experiemtn, and it was robust to it. The only thing I wish they had done is show a picture of what their predictions looked like on unlabeled data. They said it looked plausible but did not provide a picture.","i can only be as convinced as what their results show, and based on their results the random forest performed best.",
